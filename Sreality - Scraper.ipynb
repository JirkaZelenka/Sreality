{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sreality - Scraping\n",
    "## Jirka Zelenka\n",
    "### 12.3.-24.4.2020\n",
    "### Celý projekt = Scraping + Cleaning & Dropping + Vizualizace + All_In_One + PowerBI\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerekvizity:\n",
    "* chromedriver.exe v přísluěné verzi k prohlížeči\n",
    "* nainstalované package\n",
    "* přístup k souboru \"Adresy.xlsx\"\n",
    "* kvalitní připojení k webu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsah:\n",
    "* 1) Importování Packagů\n",
    "* 2) Scraping URLs\n",
    "* 3) JSON - souřadnice, popis, cena\n",
    "* 4) Získávání zbylých informací\n",
    "* 5) Mapping adres - předešlé inzeráty + Nominatim GeoPy\n",
    "* 6) Spuštění\n",
    "* 7) BONUS - hudební vsuvka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 1) Importování Packagů\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 = Scraping ###############################################################################################################################\n",
    "\n",
    "##### Obecné ############\n",
    "import pandas as pd                     # for dataframes' manipulation\n",
    "from pandas import DataFrame            # for creating dataframes\n",
    "import numpy as np                      # for arrays\n",
    "import matplotlib as plt                # for plotting\n",
    "from matplotlib.pyplot import figure    # for saving and changing size of plots\n",
    "\n",
    "from collections import Counter         # for counting elements \n",
    "from datetime import datetime           #for actual date\n",
    "import re                               # !!! relativní Novinka - regular expressions\n",
    "from time import sleep                  # for sleeping (slowing down) inside a function\n",
    "import random                           # for random number (sleeping)\n",
    "import math                             # Round float\n",
    "import time                             # Time measuring\n",
    "import itertools                        # for unlisting nested lists\n",
    "\n",
    "\n",
    "##### Scraping ############\n",
    "import requests                         # for robots check\n",
    "from bs4 import BeautifulSoup           # for parsing\n",
    "from selenium import webdriver          # for browsers control\n",
    "import json                             # for Requests\n",
    "\n",
    "##### GeoPy ############        \n",
    "from geopy.geocoders import Nominatim   # Geolocator   # pip install geopy  \n",
    "from geopy.exc import GeocoderTimedOut  # for Error handling\n",
    "\n",
    "##########################\n",
    "# Zaítm nepoužito:\n",
    "##### Widgets ############\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import display\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "##### Bonus - Hudba ############\n",
    "import winsound                        # for Beep-sounds\n",
    "\n",
    "##### Vizualizace ############\n",
    "import seaborn as sns                  #for cool plots\n",
    "\n",
    "\n",
    "\n",
    "import sys                             # ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 2) Scraping URLs\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get(\"https://www.sreality.cz/robots.txt\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = Scraping ################################################################################################################################\n",
    "\n",
    "def get_soup_elements(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 1):  \n",
    "    \n",
    "    browser = webdriver.Chrome()\n",
    "    \n",
    "    ##########################################\n",
    "    # 1. Volba Prodej/Pronájem, Byty/Domy,                 --Aukce/Bez Aukce (jen pro Prodeje) zatím nechávám být, cpe se mi to doprostřed url\n",
    "    ##########################################   \n",
    "    \n",
    "    url_x = r\"https://www.sreality.cz/hledani\"             \n",
    "    url = url_x + \"/\" +  typ_obchodu + \"/\" +  typ_stavby\n",
    "\n",
    "    ##########################################\n",
    "    # 2. načtení webu\n",
    "    ##########################################\n",
    "    \n",
    "    browser.get(url)    # (url).text ??\n",
    "    sleep(random.uniform(1.0, 1.5))\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml') # \"parser\" ??\n",
    "    \n",
    "    elements = []    \n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^/detail/\")}):       # !!!!!!!!!!!!!!!!! změněno, protože H2 neobsahovalo všechny věci, jen nadpisek.\n",
    "        link = link.get('href')   \n",
    "        elements.append(link)     \n",
    "    elements = elements[0::2]   \n",
    "\n",
    "    ##########################################\n",
    "    # 3. zjištění počtu listů - mělo by být optional, ale nevadí\n",
    "    ##########################################\n",
    "    records = soup.find_all(class_ ='numero ng-binding')[1].text\n",
    "    records = re.split(r'\\D', str(records))                         \n",
    "    records = \",\".join(records).replace(\",\", \"\")\n",
    "    records = int(records)\n",
    "    max_page = math.ceil(records / 20)   \n",
    "    print(\"----------------\")\n",
    "    print(\"Scrapuji: \" + str(typ_obchodu) + \" \" + str(typ_stavby))\n",
    "    print(\"Celkem inzerátů: \" + str(records))\n",
    "    print(\"Celkem stránek: \" + str(max_page))\n",
    "    \n",
    "    ##########################################\n",
    "    # 4. nastavení počtu stránek  -mělo být víc promakané\n",
    "    ##########################################\n",
    "    if pages == 999:      # (NE)Speciální případ, chci všechny inzeráty - standardně tu bude asi kolem 600 stránek max, takže 999 je volné k použití\n",
    "        pages = max_page\n",
    "    \n",
    "    print(\"Scrapuji (pouze) \" + str(pages) + \" stran.\")\n",
    "    print(\"----------------\")\n",
    "    \n",
    "    ##########################################\n",
    "    # 5. Scrapping zbylých listů - naštěstí v jednom okně\n",
    "    ##########################################    \n",
    "    \n",
    "    for i in range(pages-1):   \n",
    "        i = i+2\n",
    "        \n",
    "        sys.stdout.write('\\r'+ \"Strana \" + str(i-1) + \" = \" + str(round(100*(i-1)/(pages), 2)) + \"% progress. Zbývá cca: \" + str(round(random.uniform(3.4, 3.8)*(pages-(i-1)), 2 )) + \" sekund.\")    # Asi upravím čas, na rychlejším kabelu v obýváku je to občas i tak 3 sec :O\n",
    "\n",
    "        url2 = url + \"?strana=\" + str(i)\n",
    "        browser.get(url2)\n",
    "\n",
    "        sleep(random.uniform(1.0, 1.5))\n",
    "\n",
    "        innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "        soup2 = BeautifulSoup(innerHTML,'lxml') \n",
    "        \n",
    "        elements2 = []\n",
    "        \n",
    "        for link in soup2.findAll('a', attrs={'href': re.compile(\"^/detail/prodej/\")}):  \n",
    "            link = link.get('href') \n",
    "            elements2.append(link)  \n",
    "   \n",
    "        elements2 = elements2[0::2]  \n",
    "        \n",
    "        elements = elements + elements2     # tyto se už můžou posčítat, naštěstí, řpedtím než z nich budeme dělat elems = prvky třeba jména\n",
    "\n",
    "    \n",
    "    browser.quit()   \n",
    "    \n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2 = Získání URLS ###############################################################################################################################\n",
    "\n",
    "def elements_and_ids(x):\n",
    "    \n",
    "    elements = pd.DataFrame({\"url\":x})\n",
    "\n",
    "    def get_id(x):\n",
    "        x = x.split(\"/\")[-1]\n",
    "        return x\n",
    "\n",
    "    elements[\"url_id\"] = elements[\"url\"].apply(get_id)\n",
    "    \n",
    "    len1 = len(elements)\n",
    "    #Přidáno nově, v tuto chvíli odmažu duplikáty a jsem v pohodě a šetřím si čas dál.\n",
    "    elements = elements.drop_duplicates(subset = [ \"url\", \"url_id\"], keep = \"first\", inplace = False)   \n",
    "    len2 = len(elements)                                                                             \n",
    "                                                                                                      \n",
    "    print(\"-- Vymazáno \" + str(len1-len2) + \" záznamů kvůli duplikaci.\")\n",
    "    return elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 3) JSON - souřadnice, popis, cena\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 = získání Souřadnic, Ceny a Popisu = z JSON ################################################################################################################################  \n",
    "\n",
    "def get_coords_price_meters(x):\n",
    "    \n",
    "    url = \"https://www.sreality.cz/api/cs/v2/estates/\" + str(x)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    byt = json.loads(response.text, encoding = 'UTF-8')                                \n",
    "    try:\n",
    "        coords = (byt[\"map\"][\"lat\"], byt[\"map\"][\"lon\"])\n",
    "    except:\n",
    "        coords = (0.01, 0.01)  #Pro případ neexisutjících souřadnic\n",
    "    try:\n",
    "        price = byt[\"price_czk\"][\"value_raw\"] \n",
    "    except:\n",
    "        price = -1\n",
    "        \n",
    "    try:\n",
    "        description = byt[\"meta_description\"]\n",
    "    except:\n",
    "        description = -1\n",
    "    \n",
    "    return coords, price, description\n",
    "\n",
    "\n",
    "# Severní Šířka = latitude\n",
    "# výchoDní / zápaDní Délka = longitude\n",
    "\n",
    "def latitude(x):                   #Rozdělí souřadnice na LAT a LON\n",
    "    x = str(x).split()[0][1:8]\n",
    "    return x\n",
    "\n",
    "def longitude(x):\n",
    "    x = str(x).split()[1][0:7]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 4) Získávání zbylých informací\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  4 = Prodej + Dům + Pokoje = z URL ###############################################################################################################################\n",
    "\n",
    "def characteristics(x):\n",
    "    x = x.split(\"/\")\n",
    "    buy_rent = x[2]\n",
    "    home_house = x[3]\n",
    "    rooms = x[4]\n",
    "    \n",
    "    return buy_rent, home_house, rooms\n",
    "\n",
    "#  5 =  Plocha z Popisu ###############################################################################################################################\n",
    "\n",
    "# Upraveno pro čísla větší než 1000 aby je to vzalo\n",
    "# Zároveň se to vyhne velikost \"Dispozice\", \"Atpyický\", atd.\n",
    "\n",
    "def plocha(x):\n",
    "    try:\n",
    "        metry = re.search(r'\\s[12]\\s\\d{3}\\s[m]', x)[0] # SPecificky popsáno: Začíná to mezerou, pak 1 nebo 2, pak mezera, pak tři čísla, mezera a pak \"m\"\n",
    "        metry = metry.split()[0] + metry.split()[1]     # Separuju Jedničku + stovky metrů, bez \"m\"\n",
    "    except:\n",
    "        try:\n",
    "            metry = re.search(r'\\s\\d{2,3}\\s[m]', x)[0]  #Mezera, pak 1-3 čísla, mezera a metr\n",
    "            metry = metry.split()[0]                    # Separuju čísla, bez \"m\"\n",
    "        except:\n",
    "            metry = -1\n",
    "    return metry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 5) Mapping adres - předešlé inzeráty + Nominatim GeoPy\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Státní správa\n",
    "    https://www.statnisprava.cz/rstsp/ciselniky.nsf/i/CZ0201\n",
    "    - problémy s abecedou, Brno-město nemá obce atd\n",
    "### 2) Wiki\n",
    "    https://cs.wikipedia.org/wiki/Seznam_katastr%C3%A1ln%C3%ADch_%C3%BAzem%C3%AD_v_okrese_Bene%C5%A1ov\n",
    "    - taky celý dost na houby, nezačal jsem\n",
    "### 3) Volby.cz\n",
    "    https://www.volby.cz/pls/kv2018/kv31?xjazyk=CZ&xid=1\n",
    "    - Bylo by hezké, jsou to tables, easy to scrapp, jen jiný počet než v 1) (75 vs 77 okresů)\n",
    "### 4) Staťák - excel\n",
    "    https://www.czso.cz/csu/czso/pocet-obyvatel-v-obcich-za0wri436p?fbclid=IwAR1haSspuynZB8Awn08WDriMkUcsUCz4fHH9Pw2CwMDVGHPGJERxaqbrVg8\n",
    "    - Tohle asi bude top, Zuzka mě zachránila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 14 krajů\n",
    "* 77 okresů podle státní správy včetně PRAHA\n",
    "* 6258 obcí a újezdů  K 27. květnu 2016 (ČSÚ)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 = Adresy z předešlých inzerátů a short_coords ###############################################################################################################################\n",
    "\n",
    "# Vytvoření ořezaných souřadnic, přesnost je dostatečná, lépe se najdou duplikáty\n",
    "def short_coords(x):\n",
    "    \"\"\"\n",
    "    x = x.astype(str)   # Bylo potřeba udělat string - ale Tuple se blbě převádí - vyřešil jsem uložením a načtením skrz excel\n",
    "    \"\"\"\n",
    "    \n",
    "    x1 = re.split(r'\\W+', x)[1] + \".\"+re.split(r'\\W+', x)[2]\n",
    "    x1 = round(float(x1), 4)\n",
    "\n",
    "    x2 = re.split(r'\\W+', x)[3] + \".\"+re.split(r'\\W+', x)[4]\n",
    "    x2 = round(float(x2), 4)\n",
    "\n",
    "    return (x1, x2)\n",
    "\n",
    "#############################\n",
    "\n",
    "# Napmapuje až 80 % Adres z předešlých inzerátů\n",
    "def adress_old(x):  \n",
    "\n",
    "    adresy = pd.read_excel(\"Adresy.xlsx\")\n",
    "    adresy = adresy[[\"oblast\", \"město\", \"okres\", \"kraj\", \"url_id\", \"short_coords\"]]\n",
    "    \n",
    "    #Nejlepší napárování je toto:\n",
    "    # alternativně Inner a Left minus řádky s NaNs a funguje stejně)\n",
    "    \n",
    "    x.short_coords = x.short_coords.astype(str)                              # získat string na souřadnice, protože v Načteném adresáři je mám už taky jako string\n",
    "    data = pd.merge(x, adresy, on=[\"short_coords\", \"url_id\"], how = \"left\")  #upraveno matchování na url_ID + short_coords, je to tak iv Adresáří, je to jednoznačné, jsou tam unikátní. \n",
    "                                                                            # Pokud si v dalším kroku dostáhnu ke starému url_id a k nové coords ještě novou adresu, tak pak se mi uloží do Adresáře nová kombiance ID + short_coord a je to OK\n",
    "                                                                             # Viz funkce\"update_databáze_adres() kde je totéž info\n",
    "            \n",
    "    print(\"-- Počet doplněných řádků je: \" + str(len(data[~data.kraj.isna()])) + \", počet chybějících řádků je: \"   + str(len(data[data.kraj.isna()])))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 = Adresy - zbývající přes GeoLocator ###############################################################################################################################\n",
    "\n",
    "def adress_new(x):\n",
    "\n",
    "# Pozn. - je to random, závislost rychlosti na user_agent, i na format_string se nepovedlo potvrdit - ale dokumentace user-agent uvíádí jako povinnost\n",
    "# Timeout na 20s  zrušil Errory - None záleží na verzi geopy !! viz dokumentace\n",
    "#Rychlost a úspěch velmi záleží na připojení. Ryhlost 0.2s - 10s na záznam.\n",
    "# Problém s Too many requests se \"spraví přes noc\", kdyžtak - nebo viz stackoverflow - nastavit user-agent (https://stackoverflow.com/questions/22786068/how-to-avoid-http-error-429-too-many-requests-python)\n",
    "  \n",
    "    geolocator = Nominatim(timeout = 20, user_agent = \"JZ_Sreality\")   # Pomohlo změnit jméno, proti \"Error 403\" !!        \n",
    "    location = geolocator.reverse(x.strip(\"())\"))   \n",
    "                                                    # Reverse samotné znamená obrácené vyhleádvání = souřadnice -> Adresa\n",
    "    try:\n",
    "        oblast  = location[0].split(\",\")[-7]\n",
    "    except:\n",
    "        oblast  = -1\n",
    "    try:\n",
    "        město = location[0].split(\",\")[-6]\n",
    "    except:\n",
    "        město  = -1\n",
    "    try:\n",
    "        okres = location[0].split(\",\")[-5]\n",
    "    except:\n",
    "        okres  = -1\n",
    "    try:\n",
    "        kraj = location[0].split(\",\")[-4]  \n",
    "    except:\n",
    "        kraj  = -1       \n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    return oblast, město, okres, kraj\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# Pomocná funkce, opakuje předchozí funkci pořád dokola dokud neprojde bez Erroru\n",
    "def repeat_adress(x):\n",
    "    try:\n",
    "        x[\"oblast\"], x[\"město\"],  x[\"okres\"] ,  x[\"kraj\"]  = zip(*x['coords'].map(adress_new))\n",
    "    except GeocoderTimedOut:\n",
    "        print(\"Another try\")\n",
    "        x[\"oblast\"], x[\"město\"],  x[\"okres\"] ,  x[\"kraj\"]  = zip(*x['coords'].map(adress_new))\n",
    "\n",
    "\n",
    "# 8 = Merging adres ###############################################################################################################################\n",
    "# Aplikuje předchozí funkci pouze na řídky, které ještě nemají doplněné adresy z kroku 6.)\n",
    "\n",
    "def adress_merging(x):\n",
    "\n",
    "    data_new = x.copy()          \n",
    "    bool_series = pd.isnull(data_new.kraj)                                   \n",
    "    data_new = data_new[bool_series]     #subset s chybějícími adresami   \n",
    "        \n",
    "    repeat_adress(data_new)\n",
    "        \n",
    "    data_all = pd.concat([x, data_new], join_axes=[x.columns])   \n",
    "    data_all = data_all[~pd.isnull(data_all.kraj)]\n",
    "    data_all = data_all.sort_index()\n",
    "\n",
    "    data = data_all.copy()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 6) Spuštění\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parametry:\n",
    "# \"prodej\"/ \"pronájem\"\n",
    "# \"byty\"/\"domy\"\n",
    "# pages = 1- X, případně 999 = Všechny strany !\n",
    "\n",
    "def scrap_all(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 1):\n",
    "    \n",
    "    # Scrapni data - hezky komunikuje = cca 50 min\n",
    "    data = get_soup_elements(typ_obchodu = typ_obchodu, typ_stavby = typ_stavby, pages = pages)\n",
    "    print( \"1/8 Data scrapnuta, získávám URLs.\")\n",
    "    \n",
    "    # 2 = Získání URLS\n",
    "    data = elements_and_ids(data)\n",
    "    data.to_excel(r\"a1_URLs_prodej_byty.xlsx\")\n",
    "    print( \"2/8 Získány URL, nyní získávám Souřadnice, Ceny a Popis - několik minut...\")\n",
    "    \n",
    "    \n",
    "    # 3 = získání Souřadnic, Ceny a Popisu = z JSON\n",
    "    data[\"coords\"], data[\"cena\"], data[\"popis\"] = zip(*data['url_id'].map(get_coords_price_meters))\n",
    "    data[\"lat\"] = data[\"coords\"].apply(latitude)\n",
    "    data[\"lon\"] = data[\"coords\"].apply(longitude)\n",
    "    data.to_excel(r\"a2_Souřadnice_prodej_byty.xlsx\")\n",
    "    print( \"3/8 Získány Souřadnice, Ceny a Popis, nyní získávám informace z URLs.\")\n",
    "   \n",
    "    # 4 = Prodej + Dům + Pokoje = z URL\n",
    "    data[\"prodej\"], data[\"dům\"],  data[\"pokoje\"] = zip(*data['url'].map(characteristics))\n",
    "    print(\"4/8 Získány informace z URLs, nyní získávám informace z popisu.\")\n",
    "    \n",
    "    # 5 =  Plocha z Popisu\n",
    "    data[\"plocha\"] = data['popis'].apply(plocha)\n",
    "    data.to_excel(r\"a3_Popisky_prodej_byty.xlsx\")\n",
    "    print( \"5/8 Získány informace z Popisu, nyní mapuji Adresy z předešlých inzerátů.\")\n",
    "    \n",
    "   \n",
    "    # 6 = Adresy z předešlých inzerátů a short_coords\n",
    "    data = pd.read_excel(r\"a3_Popisky_prodej_byty.xlsx\")   # Abych se vyhnul konverzi TUPLE na STRING, což není triviální, tak si to radši uložím a znova načtu a získám stringy rovnou. Snad mi to nerozbije zbytek\n",
    "    data[\"short_coords\"] = data[\"coords\"].apply(short_coords)\n",
    "    data_upd = adress_old(data)                                # Tady nepotřebuji maping, protože se nesnažím něco nahodit na všechny řádky, ale merguju celé datasety\n",
    "    data = data_upd.copy()\n",
    "    print( \"6/8 Namapovány Adresy z předešlých inzerátů, nyní stahuji nové Adresy - několik minut...\")            # Přidat do printu počet řádků, kolik mám a kolik zbývá v 7. kroku\n",
    "\n",
    "    # 7-8 = Adresy - zbývající přes GeoLocator + Merging\n",
    "\n",
    "    try:                                    # !!! Riskuju že zas něco selže, jako USER- AGENT posledně...\n",
    "        data_upd = adress_merging(data)    #Přidáno TRY pro situace, kdy už mám všechyn adresy z OLD a nejde nic namapovat !\n",
    "        data = data_upd.copy()\n",
    "        data.to_excel(r\"a4_SCRAPED_prodej_byty.xlsx\")\n",
    "        print(\"7+8/8 Získány nové adresy + mergnuto dohromady. Celková délka datasetu: \"+ str(len(data)) + \". Konec Fáze 1.\")\n",
    "    \n",
    "    except:\n",
    "        data.to_excel(r\"a4_SCRAPED_prodej_byty.xlsx\")\n",
    "        print(\"7+8/8 ŽÁDNÉ nové adresy. Celková délka datasetu: \"+ str(len(data)) + \". Konec Fáze 1.\")\n",
    "    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Scrapuji: prodej byty\n",
      "Celkem inzerátů: 14050\n",
      "Celkem stránek: 703\n",
      "Scrapuji (pouze) 2 stran.\n",
      "----------------\n",
      "Strana 1 = 50.0% progress. Zbývá cca: 3.5 sekund.1/8 Data scrapnuta, získávám URLs.\n",
      "-- Vymazáno 0 záznamů kvůli duplikaci.\n",
      "2/8 Získány URL, nyní získávám Souřadnice, Ceny a Popis - několik minut...\n",
      "3/8 Získány Souřadnice, Ceny a Popis, nyní získávám informace z URLs.\n",
      "4/8 Získány informace z URLs, nyní získávám informace z popisu.\n",
      "5/8 Získány informace z Popisu, nyní mapuji Adresy z předešlých inzerátů.\n",
      "-- Počet doplněných řádků je: 22, počet chybějících řádků je: 18\n",
      "6/8 Namapovány Adresy z předešlých inzerátů, nyní stahuji nové Adresy - několik minut...\n",
      "7+8/8 Získány nové adresy + mergnuto dohromady. Celková délka datasetu: 40. Konec Fáze 1.\n"
     ]
    }
   ],
   "source": [
    "data = scrap_all(pages=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS - Hudební vsuvka aplikovatelná do scrapingu jako alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "import time \n",
    "\n",
    "sec = 300  # milliseconds - for Beep\n",
    "half_sec = 150\n",
    "pause = 1   # Seconds - for Sleep\n",
    "\n",
    "# záhadně to nebere násobky floatem ani dělení integerem i když ot ve výsledku je celé číslo\n",
    "\n",
    "# Hz\n",
    "A3 = 220\n",
    "C4 = 262   #261.63\n",
    "D4 = 294   #293.67\n",
    "E4 = 330   #329.63\n",
    "F4 = 350   #349.23\n",
    "G4 = 392\n",
    "A4 = 440\n",
    "C5 = 523   #523.25\n",
    "\n",
    "koef = 2\n",
    "\n",
    "def we_are_the_champions():\n",
    "     \n",
    "    winsound.Beep(koef*F4, 4*sec)\n",
    "    winsound.Beep(koef*E4, sec)\n",
    "    winsound.Beep(koef*F4, sec)\n",
    "    winsound.Beep(koef*E4, 3*sec)\n",
    "    winsound.Beep(koef*C4, 2*sec)\n",
    "    winsound.Beep(koef*A3, 3*half_sec)\n",
    "    winsound.Beep(koef*D4, 3*half_sec)\n",
    "    winsound.Beep(koef*A3, 10*sec)\n",
    "    time.sleep(pause)\n",
    "    winsound.Beep(koef*C4, sec)\n",
    "    winsound.Beep(koef*F4, 4*sec)\n",
    "    winsound.Beep(koef*G4, 2*sec)\n",
    "    winsound.Beep(koef*A4, 2*sec)\n",
    "    winsound.Beep(koef*C5, 4*sec)\n",
    "    winsound.Beep(koef*A4, 2*sec)\n",
    "    winsound.Beep(koef*D4, sec)\n",
    "    winsound.Beep(koef*E4, sec)\n",
    "    winsound.Beep(koef*D4, 6*sec)\n",
    "\n",
    "    \n",
    "    \n",
    "we_are_the_champions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
